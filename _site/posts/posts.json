[
  {
    "path": "posts/2021-04-15-rayshade-precipitation/",
    "title": "Rayshading Precipitation Maps",
    "description": "I rendered some preciptiation maps of the contiguous US with rayshader.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\r\nRecently, I’ve been exploring some different gridded daily precipitation datasets and evaluating how they impact watershed models I’ve been working on. Let’s explore using the rayshader to make some three-dimensional maps.\r\nFirst, obtain the gridded precipitation data. I will use the 30-yr mean annual precipitation from PRISM. Luckily, there is an R package so we can easily script the data download.\r\nSecond, I will smooth the data a little bit by summarizing into tiles. There is a lot of variation at the nation-wide scale by spatially-binning and averaging the data, the map is just a little more aesthetically pleasing to my eye. This can be done using base and sp or with ggplot. I am going to demonstrate both workflows.\r\nThird, I will render a three-dimensional map with rayshader. So, load the necessary packages before we start. I’m using the development version of rayshader and rayrender because of some massive speed improvements, I recommend installing if you can (remotes::install_github(tylermorganwall/rayshader); remotes::install_github(tylermorganwall/rayrender)). statesRcontiguous is only on Github, so install as follows: remotes::install_github(charliejhadley/statesRcontiguous).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rayshader) ## using v0.24.6 from Github\r\nlibrary(prism) ## to download PRISM data\r\nlibrary(raster)\r\nlibrary(statesRcontiguous) ## shapefiles for clipping data\r\nlibrary(sf)\r\n\r\n\r\n\r\nDownload Data\r\nThe prism package downloads the gridded precipitation datasets we request, then stores the path so we can read with the raster function. I’m downloading the 4km resolution data, if you were doing this at the state or local level, consider downloading the 800m dataset for better resolution.\r\nI also need to do some data cropping/clipping. The statesRcontigous package has some handy shapefiles of the US and states to facilitate the process. It is only on github, so head over and download it.\r\n\r\n\r\n## PRISM data\r\ntmpdir <- tempdir()\r\nprism_set_dl_dir(tmpdir)\r\nprism <- get_prism_normals(\"ppt\", \"4km\", annual = TRUE, keepZip = FALSE)\r\n\r\n\r\n\r\n  |                                                                  \r\n  |                                                            |   0%\r\n  |                                                                  \r\n  |============================================================| 100%\r\n\r\nprism_rast <- prism_archive_subset(\"ppt\", \"annual normals\", resolution = \"4km\")\r\nprism_rast <- pd_to_file(prism_rast)\r\nprism_rast <- raster(prism_rast)\r\n\r\n# Lower 48 boundaries\r\nshp_contiguous_states <- shp_all_us_states %>%\r\n  dplyr::filter(contiguous.united.states) %>%\r\n  dplyr::filter(!is.na(affgeoid)) %>%\r\n  as_Spatial() %>%\r\n  aggregate()\r\n\r\n\r\n\r\nTile rasters\r\nThere are two possible workflows here. I originally used functions from the sp package to spatially summarize data. However [@cstats1](https://twitter.com/cstats1) posted an efficient workflow using ggplot that I will replicate here.\r\npreserved8a5abdbd6ff7f24\r\nThe ggplot2 workflow: Convert the raster to a data frame, then use stat_summary_2d() to spatially bin a calculate the mean precip per bin. I originally wanted hexagons (using stat_summary_hex()), but kept getting weird artifacts in the plots. So square tiles it is.\r\nThe base/sp workflow: Using sp, create a hexagonal spatial polygons object that covers our raster area. Then extract the mean precipitation in each hexagon and convert the hexagons into a raster with the mean precipitation as the value.\r\nYou will notice the first workflow is much faster. The raster::extract function run particularly slow. I suspect that the new terra package would do this much faster, but I haven’t switched my workflows over to it yet.\r\nggplot:\r\n\r\n\r\ndf <- as.data.frame(as(prism_rast, \"SpatialPixelsDataFrame\"))\r\ncolnames(df) <- c(\"value\", \"x\", \"y\")\r\np1 <- ggplot(df, aes(x, y, z = value)) +\r\n  stat_summary_2d(binwidth = c(.2,.2)) +\r\n  scale_fill_viridis_c(\"Mean Annual Precipitation [mm]\", direction = -1) +\r\n  labs(x = \"Longitude\", y = \"Latitude\", caption = \"@mpschramm\") +\r\n  theme(text = element_text(family = \"Source Sans Pro\"),\r\n        legend.position = \"bottom\",\r\n        legend.title = element_text(size = 8),\r\n        legend.text = element_text(size = 7),\r\n        legend.key.height = unit(0.25, \"cm\"),\r\n        panel.background = element_rect(fill = \"white\", color = \"white\"),\r\n        panel.grid = element_blank(),\r\n        axis.title.x = element_text(size = 6, hjust = 0),\r\n        axis.title.y = element_text(size = 6, hjust = 0),\r\n        axis.text.x = element_text(size = 6),\r\n        axis.text.y = element_text(size = 6))\r\n  \r\np1\r\n\r\n\r\n\r\n\r\nbase/sp:\r\n\r\n\r\nhexagons <- sp::spsample(x =  shp_contiguous_states,\r\n                         n = 10000,\r\n                         type = \"hexagonal\")\r\n\r\nhexagons <- sp::HexPoints2SpatialPolygons(hexagons)\r\n\r\nrow.names(hexagons) <- as.character(1:length(hexagons))\r\n\r\nhexagons <- st_intersection(st_as_sf(hexagons),\r\n                            st_as_sf(shp_contiguous_states))\r\n\r\nhexagons <- as_Spatial(hexagons)\r\n\r\nprism_summary <- raster::extract(x = prism_rast,\r\n                                     y = as(hexagons, 'SpatialPolygons'),\r\n                                     fun = mean,\r\n                                     na.rm = TRUE,\r\n                                 sp = TRUE)\r\n\r\nppt_raster <- rasterize(prism_summary,\r\n                        prism_rast,\r\n                        field = \"PRISM_ppt_30yr_normal_4kmM2_annual_bil\")\r\npal <- hcl.colors(100, palette = \"viridis\", rev = TRUE)\r\nplot(ppt_raster, col = pal)\r\n\r\n\r\n\r\n\r\nRayshade\r\nTime to make the computer go bananas. I usually render a low res version first to make sure I like the colors, scaling, etc. These can take quite some time to render. Play with the settings to get what you like and share on Twitter with #rayshader.\r\nggplot2\r\n\r\n\r\nplot_gg(p1, \r\n        multicore = TRUE, \r\n        width = 4*1.777, \r\n        height = 4,\r\n        solidcolor = \"white\",\r\n        theta = 0,\r\n        phi = 80,\r\n        fov = 0,\r\n        zoom = .5,\r\n        background = \"grey80\",\r\n        windowsize = c(1920,1080))\r\n\r\nrender_highquality(lightdirection = 45, \r\n                   lightaltitude = 60,\r\n                   lightintensity = 1000,\r\n                   samples = 2000,\r\n                   sample_method = \"sobol\",\r\n                   parallel = TRUE,\r\n                   width = 1920,\r\n                   height = 1080,\r\n                   ground_material = rayrender::diffuse(color = \"grey40\"),\r\n                   clear = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\nbase/sp\r\n\r\n\r\n## convert ras to a matrix:\r\nelmat = raster_to_matrix(ppt_raster)\r\nelmat %>%\r\n  height_shade(texture = hcl.colors(100, palette = \"viridis\", rev = TRUE)) %>%\r\n  plot_3d(elmat,\r\n          zscale = 16,\r\n          solidcolor = \"white\",\r\n          theta = 0,\r\n          phi = 80,\r\n          fov = 45,\r\n          zoom = .5,\r\n          background = \"grey80\",\r\n          windowsize = c(1920,1080))\r\nrender_highquality(lightdirection = 45, \r\n                   lightaltitude = 60,\r\n                   lightintensity = 900,\r\n                   samples = 3000,\r\n                   sample_method = \"sobol\",\r\n                   parallel = TRUE,\r\n                   width = 1920,\r\n                   height = 1080,\r\n                   ground_material = rayrender::diffuse(color = \"grey40\"),\r\n                   clear = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-15-rayshade-precipitation/rayshade-precipitation_files/figure-html5/ggplot-flow1-1.png",
    "last_modified": "2021-04-16T16:50:58-05:00",
    "input_file": "rayshade-precipitation.utf8.md",
    "preview_width": 3200,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-10-08-rayshading_maps/",
    "title": "Rayshading maps",
    "description": "An adventure in using rayshader.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-10-09",
    "categories": [],
    "preview": "posts/2020-10-08-rayshading_maps/2020-10-08-rayshading_maps_files/figure-html5/finalmap-1.png",
    "last_modified": "2020-10-09T14:14:33-05:00",
    "input_file": "2020-10-08-rayshading_maps.utf8.md",
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2020-09-09-salinity_functions/",
    "title": "Predicting estuarine salinity using simple statistical models part 2",
    "description": "Part one of some statistical approaches for estimating estuarine salinity using freshwater inflow.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-09-17",
    "categories": [],
    "preview": "posts/2020-09-09-salinity_functions/2020-09-09-salinity_functions_files/figure-html5/finalfit-1.png",
    "last_modified": "2020-09-17T23:11:27-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-24-salinity_functions/",
    "title": "Predicting estuarine salinity using simple statistical models part 1",
    "description": "Part one of some statistical approaches for estimating estuarine salinity using freshwater inflow.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-09-09",
    "categories": [],
    "preview": "posts/2020-08-24-salinity_functions/2020-08-24-salinity_functions_files/figure-html5/dataexplore-1.png",
    "last_modified": "2020-09-09T13:01:48-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-07-plotting-flood-frequency/",
    "title": "Plotting flood probability",
    "description": "Communicate flood probability in relatable terms.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-08-08",
    "categories": [],
    "preview": "posts/2020-08-07-plotting-flood-frequency/2020-08-07-plotting-flood-frequency_files/figure-html5/plot-1.png",
    "last_modified": "2020-09-09T13:01:48-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/introducing_echor/",
    "title": "Introducing echor",
    "description": "Download EPA data with R",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-09-24",
    "categories": [],
    "preview": "posts/introducing_echor/introducing_echor_files/figure-html5/finalplot1-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/binomial_water_quality/",
    "title": "Binomial Test for Water Quality Compliance",
    "description": "Use the binomial test to evaluate water quality compliance.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-04-26",
    "categories": [],
    "preview": "posts/binomial_water_quality/binomial_water_quality_files/figure-html5/output-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/date_based_rolling/",
    "title": "Date-based rolling functions",
    "description": "Apply rolling statistics to non-routine time series data.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "michaelpaulschramm.com"
      }
    ],
    "date": "2018-04-04",
    "categories": [],
    "preview": "posts/date_based_rolling/date_based_rolling_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/txwater_retweets/",
    "title": "txwater retweets",
    "description": "Let's find out the retweet relationships for txwater twitter users.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-02-21",
    "categories": [],
    "preview": "posts/txwater_retweets/txwater_retweets_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 3200,
    "preview_height": 3200
  },
  {
    "path": "posts/time-series-python/",
    "title": "Time-series decomposition and trend analysis in Python",
    "description": "Decompose time series in Python and a function for the Mann-Kendall test for trend.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2015-08-01",
    "categories": [],
    "preview": {},
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {}
  }
]
