[
  {
    "path": "posts/2020-09-09-salinity_functions/",
    "title": "Predicting estuarine salinity using simple statistical models part 2",
    "description": "Part one of some statistical approaches for estimating estuarine salinity using freshwater inflow.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2021-08-26",
    "categories": [],
    "contents": "\r\nIn the previous post I used nonlinear least squares to fit a logistic function to some salinity data. Here I explore using beta regression. Beta regression is appropriate for inherently proportional data (as opposed to proportion of success/failure or rates with shifting denominators, both covered by logistic and Poisson regression). Douma and Weedon Douma and Weedon (2019) provide a great flow chart for determining the type of model to use with proportional data. Of course, the salinity data we use comes reported in practical salinity units (psu). We can make an assumption that the estuary has some maximum salinity and ceate a response variable that is in a proportion unit. More on that in a bit. I will be using the same salinity and flow dataset loaded in the previous post.\r\nBeta regression is appropriate when the response variable continuous and restricted to \\((0,1)\\). In this dataset, salinity approaches zero but never reaches zero. The maximum value is 28.74. First we need to transform the response variable to a proportion. I will apply \\(y = \\frac{salinity}{salinity_{max} + 1}\\) to prevent \\(y=1\\). It might also be reasonable to use \\(y = \\frac{salinity}{35}\\) if we assume oceanic salinity is 35 psu. This will depend on the data and in some cases (along the South Texas coast for example) estuaries become hypersaline and this would not be appropriate. Figure 1 depicts the density distribution of the response variable. The distribution looks a little wonky and tells me that we may get a poor model fit.\r\n\r\n\r\n\r\nFigure 1: Distribution of salinity values\r\n\r\n\r\n\r\nI am using the betareg package to fit the beta regression models (Cribari-Neto and Zeileis 2009). First, a simple model using log discharge as a predictor variable is fit. betareg uses the standard formula interface of y ~ x1 + x2. The type argument indicates the estimator used (readers are referred to Simas Simas, Barreto-Souza, and Rocha (2010)). The model summary provides a psuedo R2 measure of fit, parameter estimates, precision parameter estimate, \\(\\phi\\).\r\n\r\n\r\ndf <- df %>%\r\n  mutate(\r\n    discharge = case_when(\r\n      discharge <= 0 ~ 0.0001,\r\n      discharge > 0 ~ discharge),\r\n    log_discharge = log(discharge),\r\n    y = (salinity/(max(salinity)+1)))\r\n\r\nm1 <- betareg(y ~ log_discharge,\r\n              data = df,\r\n              type = \"ML\")\r\n\r\n\r\nsummary(m1)\r\n\r\n\r\n\r\nCall:\r\nbetareg(formula = y ~ log_discharge, data = df, type = \"ML\")\r\n\r\nStandardized weighted residuals 2:\r\n    Min      1Q  Median      3Q     Max \r\n-6.5532 -0.6708  0.0779  0.6393  3.8246 \r\n\r\nCoefficients (mean model with logit link):\r\n               Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)    5.139710   0.065965   77.92   <2e-16 ***\r\nlog_discharge -0.741624   0.008137  -91.14   <2e-16 ***\r\n\r\nPhi coefficients (precision model with identity link):\r\n      Estimate Std. Error z value Pr(>|z|)    \r\n(phi)   9.1101     0.2034   44.79   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \r\n\r\nType of estimator: ML (maximum likelihood)\r\nLog-likelihood:  3156 on 3 Df\r\nPseudo R-squared: 0.3643\r\nNumber of iterations: 15 (BFGS) + 2 (Fisher scoring) \r\n\r\nA quick peak at the regression residuals should tell us a little about the model (Fig. 2). It appears the residuals are slightly biased. The data at low discharge/high salinity conditions is sparse. There are some possible patterns in the plots which suggests missing covariates. The resulting discharge-salinity plot shows the model struggling to incorporate the extreme values. I should note, that the raw data wasn’t cleaned or validated. If I were doing this for a real project I’d have to inspect if those are real values or not. In this case, I have no idea so I am leaving them in.\r\n\r\n\r\n\r\nFigure 2: Inspection of model residuals for simple regression model.\r\n\r\n\r\n\r\n\r\n\r\ndf %>%\r\n  mutate(fits = fitted(m1)) %>%\r\n  ggplot() +\r\n  geom_point(aes(date, y, color = \"Observed\"), alpha = 0.2) +\r\n  geom_step(aes(date, fits, color = \"Model Fit\"), na.rm = TRUE) +\r\n  labs(x = \"log discharge\", y = \"Salinity (proportion)\")  +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub()\r\n\r\n\r\n\r\n\r\nOne reason I am exploring this approach is that we can easliy add additional covariates to the model. I suspect seasonal and long term predictors might improve the model. So I am adding terms for day of year as a seasonal predictor and decimal date as a long-term predictor.\r\n\r\n\r\ndf <- df %>%\r\n  mutate(doy = lubridate::yday(date),\r\n         decdate = lubridate::decimal_date(date))\r\n\r\nm2 <- betareg(y ~ log_discharge + doy + decdate + decdate:doy,\r\n              data = df,\r\n              type = \"ML\")\r\n\r\nsummary(m2)\r\n\r\n\r\n\r\nCall:\r\nbetareg(formula = y ~ log_discharge + doy + decdate + decdate:doy, \r\n    data = df, type = \"ML\")\r\n\r\nStandardized weighted residuals 2:\r\n    Min      1Q  Median      3Q     Max \r\n-6.8992 -0.6472  0.0783  0.6200  3.4958 \r\n\r\nCoefficients (mean model with logit link):\r\n                Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)   -4.537e+00  1.535e+01  -0.296    0.768    \r\nlog_discharge -6.838e-01  8.447e-03 -80.952  < 2e-16 ***\r\ndoy            2.828e-01  6.854e-02   4.127 3.68e-05 ***\r\ndecdate        4.364e-03  7.629e-03   0.572    0.567    \r\ndoy:decdate   -1.394e-04  3.403e-05  -4.096 4.20e-05 ***\r\n\r\nPhi coefficients (precision model with identity link):\r\n      Estimate Std. Error z value Pr(>|z|)    \r\n(phi)   9.9342     0.2229   44.57   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \r\n\r\nType of estimator: ML (maximum likelihood)\r\nLog-likelihood:  3325 on 6 Df\r\nPseudo R-squared: 0.4083\r\nNumber of iterations: 43 (BFGS) + 14 (Fisher scoring) \r\n\r\n\r\n\r\ndf_beta <- df %>%\r\n  mutate(fits = fitted(m2),\r\n         resid = resid(m2))\r\n\r\n\r\ndf %>%\r\n  mutate(fits = fitted(m2)) %>%\r\n  ggplot() +\r\n  geom_point(aes(date, y, color = \"Observed\"), alpha = 0.2) +\r\n  geom_step(aes(date, fits, color = \"Model Fit\")) +\r\n  labs(x = \"log discharge\", y = \"Salinity (proportion)\")  +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub()\r\n\r\n\r\n\r\n\r\nI skipped over inspecting the model residuals here as I just wanted to demonstrate the benefit of using beta regression over the NLS method described in the previous post. In the next post I’ll take a look at fitting a flexible generalized additve model to the data.\r\n\r\n\r\n\r\nCribari-Neto, Francisco, and Achim Zeileis. 2009. “Beta Regression in r.”\r\n\r\n\r\nDouma, Jacob C., and James T. Weedon. 2019. “Analysing Continuous Proportions in Ecology and Evolution: A Practical Introduction to Beta and Dirichlet Regression.” Methods in Ecology and Evolution 10 (9): 1412–30. https://doi.org/10.1111/2041-210X.13234.\r\n\r\n\r\nSimas, Alexandre B, Wagner Barreto-Souza, and Andréa V Rocha. 2010. “Improved Estimators for a General Class of Beta Regression Models.” Computational Statistics & Data Analysis 54 (2): 348–66.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-09-09-salinity_functions/2020-09-09-salinity_functions_files/figure-html5/finalfit-1.png",
    "last_modified": "2021-08-26T07:15:06-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-15-rayshade-precipitation/",
    "title": "Rayshading Precipitation Maps",
    "description": "I rendered some preciptiation maps of the contiguous US with rayshader.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2021-04-15",
    "categories": [],
    "contents": "\r\nRecently, I’ve been exploring some different gridded daily precipitation datasets and evaluating how they impact watershed models I’ve been working on. Let’s explore using the rayshader to make some three-dimensional maps.\r\nFirst, obtain the gridded precipitation data. I will use the 30-yr mean annual precipitation from PRISM. Luckily, there is an R package so we can easily script the data download.\r\nSecond, I will smooth the data a little bit by summarizing into tiles. There is a lot of variation at the nation-wide scale by spatially-binning and averaging the data, the map is just a little more aesthetically pleasing to my eye. This can be done a bunch of different ways. Here, I will use the sf and terra packages to create and populate the hexagons with mean annual precipitation data.\r\nThird, I will render a three-dimensional map with rayshader. So, load the necessary packages before we start. I’m using the development version of rayshader and rayrender because of some massive speed improvements, I recommend installing if you can (remotes::install_github(tylermorganwall/rayshader); remotes::install_github(tylermorganwall/rayrender)). statesRcontiguous is only on Github, so install as follows: remotes::install_github(charliejhadley/statesRcontiguous).\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(rayshader) ## using v0.24.6 from Github\r\nlibrary(prism) ## to download PRISM data\r\nlibrary(terra)\r\nlibrary(sf)\r\n\r\n\r\n\r\nDownload Data\r\nThe prism package downloads the gridded precipitation datasets we request, then stores the path so we can read with the rast function. I’m downloading the 4km resolution data, if you were doing this at the state or local level, consider downloading the 800m dataset for better resolution.\r\n\r\n\r\n## PRISM data\r\ntmpdir <- tempdir()\r\nprism_set_dl_dir(tmpdir)\r\nprism <- get_prism_normals(\"ppt\", \"4km\", annual = TRUE, keepZip = FALSE)\r\n\r\n\r\n\r\n  |                                                                  \r\n  |                                                            |   0%\r\n  |                                                                  \r\n  |============================================================| 100%\r\n\r\nprism_rast <- prism_archive_subset(\"ppt\", \"annual normals\", resolution = \"4km\")\r\nprism_rast <- pd_to_file(prism_rast)\r\nprism_rast <- rast(prism_rast)\r\nprism_rast <- project(prism_rast, \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs \")\r\n\r\n\r\n\r\nTile rasters\r\nAs I mentioned, there are various ways to make the tiles.[@cstats1](https://twitter.com/cstats1) posted an efficient workflow using the summary_stat function in ggplot. I wanted to project the data and utilize the new terra package to summarize the data. It take a bit longer, but I like the results.\r\n\r\nSince several people have asked and I have no idea when I will actually get around to writing this process up, here are the steps to making elevation tiles in #rayshader sans pulling in the shapefiles and elevation data directly from #rstats using the Monterey Bay elevation file pic.twitter.com/0aeXnCEIk0— newishtodc (@cstats1) February 22, 2021 \r\n\r\n## create and extent polygon\r\nprism_ext <- as.polygons(ext(prism_rast), crs=crs(prism_rast))\r\nprism_ext <- st_as_sf(prism_ext)\r\n\r\n## create a hexagon grid in the extent polygon\r\nhexagons <- st_make_grid(prism_ext,\r\n                         n = c(150,150),\r\n                         square = FALSE,\r\n                         crs = crs(prism_ext))\r\n\r\n## convert from sf back to terra,\r\n## we lose the crs def along the way\r\n## so set the crs again\r\nhexagons <- vect(hexagons)  \r\ncrs(hexagons) <- crs(prism_rast)\r\n\r\n## calculate the mean raster values in each polygon\r\nprism_summary <- extract(x = prism_rast,\r\n                         y = hexagons,\r\n                         fun = mean,\r\n                         na.rm = TRUE)\r\n\r\n## extract returns a matrix, need to get the data back\r\n## into hexagons \r\nvalues(hexagons) <- data.frame(ID = 1:nrow(hexagons))\r\nhexagons <- merge(hexagons, data.frame(prism_summary))\r\nvalues(hexagons) <- data.frame(prism_summary)\r\n\r\n## what does it look like?\r\nplot(hexagons, \"PRISM_ppt_30yr_normal_4kmM2_annual_bil\")\r\n\r\n\r\n\r\n\r\nRayshade\r\nTime to make the computer go bananas. First, convert hexagons to sf and plot with ggplot2. Then we can easily generate a rayshaded plot. If you want to forgo, ggplot2, convert to a raster and run through rayshader. I usually render a low res version first to make sure I like the colors, scaling, etc. These can take quite some time to render. Play with the settings to get what you like and share on Twitter with #rayshader.\r\nggplot2\r\n\r\n\r\n## convert to sf and features with NA values\r\nhexagons <- st_as_sf(hexagons) %>%\r\n  filter(!is.na(PRISM_ppt_30yr_normal_4kmM2_annual_bil))\r\n\r\n## make your ggplot, customize as needed\r\nggplot(hexagons) +\r\n  geom_sf(aes(fill = PRISM_ppt_30yr_normal_4kmM2_annual_bil), color = NA) +\r\n  scale_fill_viridis_c(\"Annual Precipition [mm]\", direction = -1) +\r\n  labs(x = \"Longitude\", y = \"Latitude\", caption = \"@mpschramm\") +\r\n  theme(text = element_text(family = \"Source Sans Pro\"),\r\n        legend.position = \"bottom\",\r\n        legend.title = element_text(size = 8),\r\n        legend.text = element_text(size = 7),\r\n        legend.key.height = unit(0.25, \"cm\"),\r\n        panel.background = element_rect(fill = \"white\", color = \"white\"),\r\n        panel.grid = element_line(color = \"grey10\",\r\n                                  size = .1),\r\n        axis.title.x = element_text(size = 6, hjust = 0),\r\n        axis.title.y = element_text(size = 6, hjust = 0),\r\n        axis.text.x = element_text(size = 6),\r\n        axis.text.y = element_text(size = 6),\r\n        axis.ticks.x = element_blank(),\r\n        axis.ticks.y = element_blank()) -> p1\r\n\r\n## make 3D ggplot\r\nplot_gg(p1, \r\n        multicore = TRUE, \r\n        width = 4*1.777, \r\n        height = 4,\r\n        solidcolor = \"white\",\r\n        theta = 0,\r\n        phi = 80,\r\n        fov = 0,\r\n        zoom = .5,\r\n        background = \"grey80\",\r\n        windowsize = c(1920,1080))\r\n\r\n## brrrrrr\r\nrender_highquality(lightdirection = 45, \r\n                   lightaltitude = 60,\r\n                   lightintensity = 1000,\r\n                   samples = 1000, #lower this to get faster rendering\r\n                   sample_method = \"sobol\",\r\n                   parallel = TRUE,\r\n                   width = 1920,\r\n                   height = 1080,\r\n                   ground_material = rayrender::diffuse(color = \"grey40\"),\r\n                   clear = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\nbase/sp\r\n\r\n\r\n## rasterize hexagons:\r\nhexagons <- vect(hexagons)\r\ncrs(hexagons) <- crs(prism_rast)\r\nhexagons <- rasterize(hexagons, prism_rast,\r\n                      field = \"PRISM_ppt_30yr_normal_4kmM2_annual_bil\")\r\n# convert from SpatRast to Raster to matrix\r\npptmat = raster_to_matrix(raster::raster(hexagons))\r\npptmat %>%\r\n  height_shade(texture = hcl.colors(100, palette = \"viridis\", rev = TRUE)) %>%\r\n  plot_3d(pptmat,\r\n          zscale = 16,\r\n          solidcolor = \"white\",\r\n          theta = 0,\r\n          phi = 80,\r\n          fov = 45,\r\n          zoom = .5,\r\n          background = \"grey80\",\r\n          windowsize = c(1920,1080))\r\nrender_highquality(lightdirection = 45, \r\n                   lightaltitude = 60,\r\n                   lightintensity = 900,\r\n                   samples = 3000,\r\n                   sample_method = \"sobol\",\r\n                   parallel = TRUE,\r\n                   width = 1920,\r\n                   height = 1080,\r\n                   ground_material = rayrender::diffuse(color = \"grey40\"),\r\n                   clear = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-15-rayshade-precipitation/rayshade-precipitation_files/figure-html5/extract-1.png",
    "last_modified": "2021-04-19T09:05:40-05:00",
    "input_file": {},
    "preview_width": 3200,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-10-08-rayshading_maps/",
    "title": "Rayshading maps",
    "description": "An adventure in using rayshader.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-10-08",
    "categories": [],
    "contents": "\r\nI stumbled onto the wonderful rayshader while trying to beef up my R spatial skills (Morgan-Wall 2020). Experimenting with rayshading is a wonderful rabbit hole. Inspired by (but certainly not capable of replicating) the re-rendering of historic maps by numerous folks on Twitter I decided to take a go at it.\r\n\r\n\r\nState of Virginia- Department of the Interior - 19573D terrain render#arcgispro #blender #Virginia #GIS #geospatial #dataviz #3dart pic.twitter.com/2OINJdYo0F— Sean Conway (@geo_spatialist) July 28, 2020\r\n Beautiful Amami Ōshima, an island in southwest Japan, Kagoshima-ken. Part of the Geological Map of Japan Series, by the Geological Survey of Japan, 1994. Used bathymetry data this time.#rayshader adventures, an #rstats tale pic.twitter.com/A7qvnKmtPx— flotsam (@researchremora) September 24, 2020 Get started\r\nI’m going to preface this with, I have no idea what I’m doing! The code my be ugly and the outputs are not perfect, certainly not on the level of a skilled designer using ArcPro and Blender. But this is for fun. I thought it would be neat to apply a rayshader to a topo map of the Big Bend area of Texas. It has a lot of relief and some interesting features, I also know USGS has some cool maps of the region. Finally, this is heavily based on the tutorial provided by the package author.\r\nData\r\nI downloaded a GeoTIFF from this cool downloader that USGS has for historic maps: https://ngmdb.usgs.gov/topoview/viewer/#10/29.3774/-103.6938. There is a geoPDF option also, I really want to use it because overlaying vectors would be awesome. I couldn’t figure out how to read them in. If anyone has tips, let me know!\r\nThe elevation data was obtained using the handy elevatr package (Hollister and Tarak Shah 2017).\r\n\r\n\r\nlibrary(raster)\r\nlibrary(rayshader)\r\nlibrary(elevatr)\r\n\r\n\r\n\r\nFirst, download the TIFF. You can use the downloader, or the direct link below if you want to recreate what I did.\r\n\r\n\r\ntopo_map <- raster::brick(\"https://prd-tnm.s3.amazonaws.com/StagedProducts/Maps/HistoricalTopo/GeoTIFF/TX/TX_Chisos%20Mountains_122109_1985_100000_geo.tif\")\r\ntopo_map <- raster::stack(topo_map)\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow we can get the elevation data using elevatr:\r\n\r\n\r\nelevation <- get_elev_raster(raster(topo_map), z = 9)\r\n\r\n\r\n\r\n\r\n\r\n\r\nClean up the elevation raster\r\nNow we need to line up the elevation data and the topo map. I don’t want the rayshading to extend past the neatline on the map.1 So we are going to crop the elevation data to the neatline extents, then fill the remaining extent to a value somewhere in between the min and max elevation. I chose 450.\r\n\r\n\r\n## crop elevation to the full map extent (past neatline)\r\nelevation <- raster::crop(elevation, extent(topo_map))\r\n\r\n##this raster will help knockdown the elevation outside the\r\n## neatline in the physical map\r\nbase_raster <- elevation * 0 + 450\r\n\r\n## I want to crop the elevation raster to the neatlines\r\n\r\nx <- c(-104.000, -104.000, -103.000, -103.000)\r\ny <- c(29.000, 29.500, 29.000, 29.500)\r\nxy <- cbind(x,y)\r\nS <- SpatialPoints(xy, proj4string = CRS(\"+proj=longlat +ellps=clrk66 +datum=NAD27 +no_defs \"))\r\n\r\nS <- spTransform(S, crs(topo_map))\r\n\r\ninterior_elevation <- raster::crop(elevation, extent(S))\r\n\r\nelevation <- merge(interior_elevation, base_raster)\r\n\r\n\r\n\r\nTopo map to matrix\r\nNow we have the elevation raster. The raw raster file we have for the topo map needs to be addressed. It will be transformed to a 3 channel RGB array that rayshader can use to “drape” the layer on top of our rendered hills.\r\n\r\n\r\nnames(topo_map) <- c(\"r\", \"g\", \"b\")\r\ntopo_r <- rayshader::raster_to_matrix(topo_map$r)\r\ntopo_g <- rayshader::raster_to_matrix(topo_map$g)\r\ntopo_b <- rayshader::raster_to_matrix(topo_map$b)\r\ntopo_rgb_array <- array(0, dim = c(nrow(topo_r), ncol(topo_r), 3))\r\n\r\ntopo_rgb_array[,,1] <- topo_r/255\r\ntopo_rgb_array[,,2] <- topo_g/255\r\ntopo_rgb_array[,,3] <- topo_b/255\r\n\r\n## the array needs to be transposed, just because.\r\n\r\ntopo_rgb_array <- aperm(topo_rgb_array, c(2,1,3))\r\n\r\n\r\n\r\nMake some maps!\r\nFirst we convert the elevation raster to a matrix using raster_to_matrix(). The ray_shade() function calculates a shadow map from the elevation matrix, the ambient_shade() calculates the Ambient Occlusion Shadow Map. I had to look this up (https://en.wikipedia.org/wiki/Ambient_occlusion), basically this makes the surface textures more realistic by calculating how dark they are based on sun exposure. Now we can pipe everything together:\r\n\r\n\r\nelev_mat <- raster_to_matrix(elevation)\r\nray_shadow <- ray_shade(elev_mat, sunaltitude = 40, zscale = 30, multicore = TRUE)\r\nambient_shadow <- ambient_shade(elev_mat, zscale = 30)\r\n\r\nelev_mat %>%\r\n  sphere_shade() %>%\r\n  add_overlay(topo_rgb_array) %>%\r\n  add_shadow(ray_shadow, max_darken = 0.7) %>%\r\n  add_shadow(ambient_shadow, 0.25) %>%\r\n  plot_map()\r\n\r\n\r\n\r\n\r\nThe overlay (topo map) is scaled to the elevation matrix. So if we want something in higher resolution, download a higher resolution elevation matrix or take advantge of the resize_matrix() function. I am not going to cover it here for sake of processing time, but you can easily scale this up to original topo map resolution. However, if you go too large, you will run into memory allocation issues.\r\nAnimated maps\r\nFinally, by manipulating the ray_shade() function, the sunangle and sunaltitude arguments in particular, we can render shadows over the course of the day. I am not going to go into the details, Tyler Morgan Wall introduces this in this masterclass. But you can make some neat animations utilizing this function as seen below.\r\n\r\n\r\n\r\nHollister, Jeffrey, and Tarak Shah. 2017. Elevatr: Access Elevation Data from Various APIs. http://github.com/usepa/elevatr.\r\n\r\n\r\nMorgan-Wall, Tyler. 2020. Rayshader: Create Maps and Visualize Data in 2d and 3d. https://github.com/tylermorganwall/rayshader.\r\n\r\n\r\nThere is a little finger of contour line data extending past the neatline. First I think that is an extremely cool feature on this map. Second, I didn’t bother trying to rayshade that portion. But that attention to detail is why I’m not a cartographer.↩︎\r\n",
    "preview": "posts/2020-10-08-rayshading_maps/2020-10-08-rayshading_maps_files/figure-html5/finalmap-1.png",
    "last_modified": "2021-04-19T09:05:40-05:00",
    "input_file": {},
    "preview_width": 3200,
    "preview_height": 1600
  },
  {
    "path": "posts/2020-08-24-salinity_functions/",
    "title": "Predicting estuarine salinity using simple statistical models part 1",
    "description": "Part one of some statistical approaches for estimating estuarine salinity using freshwater inflow.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-09-09",
    "categories": [],
    "contents": "\r\nFor some of my projects I need to predict daily mean salinity in tidal rivers/estuaries. There are process models that are capable of doing this at daily and sub-daily time steps but require a lot of information. I rely on simple statistical methods for identifying the relationship between freshwater inflow and salinity. This article will demonstrate nonlinear least squares. The data used in this example is from USGS and the Texas Water Development Board. The dataRetrieval is a must have package if you routinely retrieve USGS stream gage data. TWDB provides a robust web API for calling data they have collated in the Water Data for Texas dashboard. minpack.lm is used for fitting nonlinear least squares. I use the tidyverse set of packages for data wrangling and plotting.\r\n\r\n\r\nlibrary(dataRetrieval)\r\nlibrary(hrbrthemes)\r\nlibrary(tidyverse)\r\nlibrary(patchwork)\r\nlibrary(minpack.lm)\r\n\r\n\r\n\r\n## downloads mean daily discharge stream gage data from USGS\r\nflow <- readNWISdv(siteNumbers = \"08041780\",\r\n                   parameterCd = \"72137\", # technically this is tidally filtered discharge\r\n                                          # 00060 is discharge\r\n                   startDate = \"2008-01-01\",\r\n                   endDate = \"2020-07-31\") %>%\r\n  dplyr::select(Date, X_72137_00003) %>%\r\n  dplyr::rename(date = Date, \r\n                discharge = X_72137_00003)\r\n\r\n\r\n## downloads hourly salinity data from TWDB\r\n## and summarizes it to mean daily salinity\r\nsalinity <- read_csv(\"https://waterdatafortexas.org/coastal/api/stations/SAB1/data/seawater_salinity?start_date=2008-01-01&end_date=2020-07-31&binning=hour&output_format=csv\",\r\n                     comment = \"#\",\r\n                     col_types = list(col_datetime(),\r\n                                      col_number())) %>%\r\n  mutate(datetime = as.Date(datetime)) %>%\r\n  group_by(datetime) %>%\r\n  summarize(salinity = mean(value))\r\n\r\ndf <- left_join(flow, salinity, by = c(\"date\" = \"datetime\")) %>%\r\n  dplyr::filter(!is.na(salinity))\r\n\r\nWith the data downloaded, a quick visualization of the data below shows the expected relationship. Increased salinity with decreased freshwater inflow and possible long term trends in flow and salinity. The salinity-flow relationship appears to be a logistic function. This makes sense since, we know salinity (at least for most estuaries under normal conditions) will have a maximum value (typically around 36 psu) and a minimum value (maybe at or above zero).\r\n\r\n\r\n## a little data exploration\r\n\r\np1 <- ggplot(df) +\r\n  geom_point(aes(date, log1p(discharge), color = \"mean daily discharge (cfs)\"), alpha = 0.25, shape = 16) +\r\n  geom_point(aes(date, salinity, color = \"mean daily salinity (psu)\"), alpha = 0.25, shape = 16) +\r\n  geom_smooth(aes(date, log1p(discharge), color = \"mean daily discharge (cfs)\"), method = \"lm\", se = FALSE) +\r\n  geom_smooth(aes(date, salinity, color = \"mean daily salinity (psu)\"), method = \"lm\", se = FALSE) +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub() +\r\n  labs(y = \"\") +\r\n  theme(legend.position = \"bottom\")\r\n\r\np2 <- ggplot(df) +\r\n  geom_point(aes(log1p(discharge), salinity), alpha = 0.25, shape = 16) +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub()\r\n\r\np1 + p2\r\n\r\n\r\nFigure 1: Flow and salinity scatterplots\r\n\r\n\r\n\r\nLinear regression\r\nSince we observe the logistic function in the streamflow salinity relationship, a linear regression probably isn’t the best choice. But let’s fit one anyways.\r\n\r\n\r\n## Go ahead a transform the predictor variable\r\n## I am also going to add a tiny amount to the single zero value\r\ndf <- df %>%\r\n  mutate(discharge = case_when(\r\n    discharge <= 0 ~ 0.0001,\r\n    discharge > 0 ~ discharge),\r\n    log_discharge = log(discharge))\r\n\r\nm.lm <- lm(salinity ~ log_discharge,\r\n           data = df)\r\nsummary(m.lm)\r\n\r\nCall:\r\nlm(formula = salinity ~ log_discharge, data = df)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-29.2695  -3.5998  -0.6124   3.2706  15.9988 \r\n\r\nCoefficients:\r\n              Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept)   26.53666    0.36913   71.89   <2e-16 ***\r\nlog_discharge -2.16244    0.04396  -49.19   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 4.834 on 3630 degrees of freedom\r\nMultiple R-squared:    0.4, Adjusted R-squared:  0.3998 \r\nF-statistic:  2420 on 1 and 3630 DF,  p-value: < 2.2e-16\r\n\r\nThe model summary indicates a fairly low adjusted r squared. I know I don’t want to use this model, but we can plot the model residuals and predictions to see where the issue is. Basically we observe over-predictions at low flows and potential under predictions at higher flows. We will move on to a nonlinear least square approach.\r\n\r\n\r\ndf_lm <- df %>%\r\n  mutate(residuals = resid(m.lm),\r\n         fits = predict(m.lm, type = \"response\"))\r\n\r\np1 <- ggplot(df_lm) +\r\n  geom_histogram(aes(residuals)) +\r\n  scale_fill_ipsum() +\r\n  theme_ipsum_pub() +\r\n  labs(subtitle = \"histogram of residuals\")\r\n\r\np2 <- ggplot(df_lm) +\r\n  geom_point(aes(log_discharge, residuals), alpha = 0.25) +\r\n  geom_hline(yintercept = 0) +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub() +\r\n  labs(subtitle = \"residuals vs predictor\")\r\n\r\np3 <- ggplot(df_lm) +\r\n  geom_point(aes(salinity, fits), alpha = 0.2) +\r\n  geom_abline(slope = 1) +\r\n  labs(x = \"measured\", y = \"predicted\",\r\n       subtitle = \"predicted vs measured\") +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub()\r\n\r\n\r\np4 <- ggplot(df_lm) +\r\n  geom_point(aes(log_discharge, salinity), alpha = 0.2) +\r\n  geom_line(aes(log_discharge, fits)) +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub() +\r\n  labs(subtitle = \"measured and fit\")\r\n\r\n\r\n(p1 + p2) / (p3 + p4)\r\n\r\n\r\nNonlinear Least Squares\r\nA clear sigmoid curve or logistic function is evident in figure… The logistic function is defined by the formula:\r\n\\[\r\nf(x) = \\frac{L}{1 + e^{-k(x-x_0)}}\r\n\\]\r\nwhere \\(L\\) = the maximum value of the curve, \\(x_0\\) = the midpoint of the curve, and \\(k\\) is the logistic growth rate. Nonlinear least squares can be used to parameterize the model. The starting values in the list are eyeballed from figure. Instead of nls, I am using the propagate package and the predictNLS function to also obtain the 95% confidence intervals.\r\n\r\n\r\nf_S <- formula(salinity ~ L / (1 + exp(-k * (log_discharge - x_0))))\r\n\r\nm.ls <- nls(f_S,\r\n              data = df,\r\n              start = list(L = 20, k = -3, x_0 = 2))\r\n\r\nsummary(m.ls)\r\n\r\nFormula: salinity ~ L/(1 + exp(-k * (log_discharge - x_0)))\r\n\r\nParameters:\r\n    Estimate Std. Error t value Pr(>|t|)    \r\nL   18.37048    0.26981   68.09   <2e-16 ***\r\nk   -1.21351    0.03357  -36.15   <2e-16 ***\r\nx_0  8.22846    0.03259  252.46   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 3.555 on 3629 degrees of freedom\r\n\r\nNumber of iterations to convergence: 8 \r\nAchieved convergence tolerance: 6.636e-06\r\n\r\ndf_ls <- propagate::predictNLS(model = m.ls, \r\n                      newdata = data.frame(log_discharge = seq(min(df$log_discharge),max(df$log_discharge), by = .1)),\r\n                      interval = \"confidence\", nsim = 10000, alpha = 0.05)\r\n\r\n\r\n\r\ndf_predict <- tibble(\r\n  log_discharge = seq(min(df$log_discharge),max(df$log_discharge), by = .1),\r\n  fit = df_ls$summary$Prop.Mean.1,\r\n  upr = df_ls$summary$`Prop.97.5%`,\r\n  lwr = df_ls$summary$`Prop.2.5%`)\r\n\r\n\r\nggplot() +\r\n  geom_point(data = df, aes(log_discharge, salinity, color = \"Measured Values\"), alpha = 0.25) +\r\n  geom_line(data = df_predict, aes(log_discharge, fit, color = \"Least Squares Fit\"), color = \"black\") +\r\n  geom_ribbon(data = df_predict, aes(x = log_discharge, ymin = lwr, ymax = upr), alpha = 0.5) +\r\n  scale_color_ipsum() +\r\n  theme_ipsum_pub()\r\n\r\n\r\n\r\n\r\n\r\nThe model summary provides the parameter estimates, \\(f(x) = \\frac{18.37}{1 + e^{--1.214(x-8.228)}}\\) The plot generally appears to follow reality. However, when we look at the model residuals below, it is evident that as salinity approaches zero, the model over predicts. If you prefer a numeric model metric, hydroGOF provides a range of model metrics to choose from below. Additional model terms to account for covariates such as season or long term trends might result in a better model. In future posts I will look at fitting this data with beta regression and generalized additive models. The NLS approach is appealing because it provides a simple model with relatively good fit. However, if we want better predictive performance I suspect a multiple regression approach would offer some advantages.\r\n\r\n\r\n\r\n\r\n\r\nhydroGOF::gof(df_ls$salinity, df_ls$fits)\r\n\r\n          [,1]\r\nME       -0.07\r\nMAE       2.73\r\nMSE      12.63\r\nRMSE      3.55\r\nNRMSE %  71.00\r\nPBIAS %  -0.80\r\nRSR       0.71\r\nrSD       1.25\r\nNSE       0.50\r\nmNSE      0.37\r\nrNSE      0.10\r\nd         0.89\r\nmd        0.72\r\nrd        0.81\r\ncp      -14.41\r\nr         0.82\r\nR2        0.68\r\nbR2       0.68\r\nKGE       0.70\r\nVE        0.69\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-24-salinity_functions/2020-08-24-salinity_functions_files/figure-html5/dataexplore-1.png",
    "last_modified": "2020-09-09T13:01:48-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-07-plotting-flood-frequency/",
    "title": "Plotting flood probability",
    "description": "Communicate flood probability in relatable terms.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2020-08-08",
    "categories": [],
    "contents": "\r\nThe public generally misunderstands the recurrence intervals used to describe flood events or storms. Typically, people believe the 100-year flood event as something that will only happen once every one hundred years. This is understandable given the description. However this recurrence interval means that there is a 1% chance of the event occurring in any given year. It might be better to explain the probability of a given flood event occurring over a time period. Say, 5 years or maybe over a 15 or 30 year mortgage. Then a person that moves into a 100-year floodplain would have some understanding of the probability of experiencing a flood event over the next n-years. We can use a binomial formula to calculate and graphically display these probabilities:\r\n\\[\r\nP(k \\text{ exceedances } n \\text{ years})=\\frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}\r\n\\]\r\nThe relationship between probability \\(p\\) and the recurrence interval \\(RI\\), \\(RI=\\frac{1}{p}\\). In R we can utilize dbinom() to calculate the exceedance probability as: dbinom(k, n, p).\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(hrbrthemes)\r\n\r\ndf <- tibble(ri = rep(c(50, 100, 500, 1000), each = 30), # return interval\r\n             n = rep(1:30, 4), # n years\r\n             aep = dbinom(1, n, 1/ri)) # exceedance probability\r\n\r\nNow plot the probability of a flood event happening over n-years:\r\n\r\n\r\nggplot(df) +\r\n  geom_step(aes(n, aep, color = as.factor(ri))) +\r\n  labs(x = \"Number of years\",\r\n       y = \"Probabililty of occurence\") +\r\n  scale_color_ft(name = \"Flood Event\", labels = c(\"50-yr flood\",\r\n                                                  \"100-yr flood\",\r\n                                                  \"500-yr flood\",\r\n                                                  \"1000-yr flood\")) +\r\n  scale_x_continuous(expand = c(0, 0)) +\r\n  theme_ft_rc() +\r\n  theme(legend.position = \"bottom\",\r\n        axis.title.y.right = element_blank())\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-07-plotting-flood-frequency/2020-08-07-plotting-flood-frequency_files/figure-html5/plot-1.png",
    "last_modified": "2020-09-09T13:01:48-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/introducing_echor/",
    "title": "Introducing echor",
    "description": "Download EPA data with R",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-09-24",
    "categories": [],
    "contents": "\r\nThe U.S. Environmental Protection Agency (EPA) provides access to facility and compliance information for registered permit holders under the Clean Air Act, Clean Water Act. The primary way for non-governmental entities to obtain this data is through the EPA Environmental and Compliance History Online (ECHO) website. Data is housed under “media-specific” programs. Relevant to this post, the National Pollutant Discharge Elimination Systems (NPDES) maintains data on pollutant discharges to waterways and the Air Facility Service (AFS) maintain data on emission to air. Gibbs (Gibbs and Simpson 2009) assess the strengths and weakness of the data collated by ECHO and provide an example of assessing environmental crime rates. While a discussion about the merits of EPA’s environmental data collection efforts and methodology are warranted, this post will discuss a new package that provides API access to the ECHO database.\r\nIntroduction and need\r\nI primarily use ECHO to obtain discharge monitoring records for wastewater and industrial discharges. Until recently, my workflow was to call or email the state environmental agency and ask for all the available permit numbers in the watershed. Some states maintain and provide a GIS file with spatial locations (this was preferred, but finding out when that file was last updated was can be difficult). Once I obtained the permit numbers, I log onto ECHO and type the permits numbers in, and individually retrieve discharge records for each facility. This requires quite a bit of clicking and typing, and is prone to error. Furthermore, there is no way to verify the records I received are correct. If I mistyped a number or received a wrong record from the agency, I have little way of catching the error.\r\nThankfully, ECHO provides web access through “GET” and REST services to provide some level of automated and reproducible data access. I recently wrote the echor package to provide access to these service in R. This was my first attempt at developing an R package and my first attempt at utilizing data APIs in a programatic way.\r\nechor provides functions to search for and obtain facility data under the NPDES, AFS, and Safe Drinking Water (SDW) program areas. Functions are also available to retrieve facility specific pollutant discharge data under the NPDES and AFS programs. Under the NPDES program, pollutant discharge reporting intervals are specific to the permit, so you may get records on quarterly, monthly, or perhaps daily basis. Records under the AFS program are returned as annual emissions.\r\nUse cases\r\nDownload facility specific discharge records\r\nIf you have a facility permit number, downloading the discharge records is pretty simple. I made every attempt to return records in “tidy” long format to make plotting and further analysis easy. If you haven’t already, install echor from CRAN.\r\n\r\n\r\ninstall.packages(\"echor\")\r\n\r\n\r\n\r\nechoGetEffluent() will return the facility reported discharges. Arguments are available to specify the permit number, date range, and parameter code (the pollutant of interest). A function is also provided to search for parameter codes: echoWaterGetParams(). In the following example I want to identify the parameter code for daily flow, then get the reported discharge for a facility that I already have a permit number for.\r\n\r\n\r\n## Load package\r\nlibrary(echor)\r\n## Find the parameter code\r\nechoWaterGetParams(term = \"Flow, in conduit\")\r\n\r\n\r\n# A tibble: 2 x 2\r\n  ValueCode ValueDescription                        \r\n  <chr>     <chr>                                   \r\n1 51725     Flow, in conduit or thru treatment plant\r\n2 50050     Flow, in conduit or thru treatment plant\r\n\r\nHere, I get two parameter codes. I happen to know I want to use parameter code 50050. Searching for the right terminology and correct code can be difficult and might require some trial and error.\r\nThe next step is to obtain the discharge records:\r\n\r\n\r\ndf <- echoGetEffluent(p_id = 'tx0119407', parameter_code = '50050',\r\n                      start_date = \"01/01/2017\", end_date = \"08/30/2018\")\r\ntibble::glimpse(df)\r\n\r\n\r\nRows: 38\r\nColumns: 61\r\n$ activity_id                 <chr> \"3600178396\", \"3600178396\", \"360~\r\n$ npdes_id                    <chr> \"TX0119407\", \"TX0119407\", \"TX011~\r\n$ version_nmbr                <chr> \"4\", \"4\", \"4\", \"4\", \"4\", \"4\", \"4~\r\n$ perm_feature_id             <chr> \"3600049681\", \"3600049681\", \"360~\r\n$ perm_feature_nmbr           <chr> \"001\", \"001\", \"001\", \"001\", \"001~\r\n$ perm_feature_type_code      <chr> \"EXO\", \"EXO\", \"EXO\", \"EXO\", \"EXO~\r\n$ perm_feature_type_desc      <chr> \"External Outfall\", \"External Ou~\r\n$ limit_set_id                <chr> \"3600061722\", \"3600061722\", \"360~\r\n$ limit_set_schedule_id       <chr> \"3600073706\", \"3600073706\", \"360~\r\n$ limit_id                    <chr> \"3600437316\", \"3600437316\", \"360~\r\n$ limit_type_code             <chr> \"ENF\", \"ENF\", \"ENF\", \"ENF\", \"ENF~\r\n$ limit_begin_date            <chr> \"08/01/2015\", \"08/01/2015\", \"08/~\r\n$ limit_end_date              <chr> \"03/01/2020\", \"03/01/2020\", \"03/~\r\n$ nmbr_of_submission          <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1~\r\n$ parameter_code              <chr> \"50050\", \"50050\", \"50050\", \"5005~\r\n$ parameter_desc              <chr> \"Flow, in conduit or thru treatm~\r\n$ monitoring_location_code    <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1~\r\n$ monitoring_location_desc    <chr> \"Effluent Gross\", \"Effluent Gros~\r\n$ stay_type_code              <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ stay_type_desc              <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ limit_value_id              <chr> \"3600678122\", \"3600678123\", \"360~\r\n$ limit_value_type_code       <chr> \"Q2\", \"Q1\", \"Q2\", \"Q1\", \"Q2\", \"Q~\r\n$ limit_value_type_desc       <chr> \"Quantity2\", \"Quantity1\", \"Quant~\r\n$ limit_value_nmbr            <chr> \"\", \".131\", \"\", \".131\", \"\", \".13~\r\n$ limit_unit_code             <chr> \"03\", \"03\", \"03\", \"03\", \"03\", \"0~\r\n$ limit_unit_desc             <chr> \"MGD\", \"MGD\", \"MGD\", \"MGD\", \"MGD~\r\n$ standard_unit_code          <chr> \"03\", \"03\", \"03\", \"03\", \"03\", \"0~\r\n$ standard_unit_desc          <chr> \"MGD\", \"MGD\", \"MGD\", \"MGD\", \"MGD~\r\n$ limit_value_standard_units  <chr> \"\", \".131\", \"\", \".131\", \"\", \".13~\r\n$ statistical_base_code       <chr> \"DD\", \"DB\", \"DD\", \"DB\", \"DD\", \"D~\r\n$ statistical_base_short_desc <chr> \"DAILY MX\", \"DAILY AV\", \"DAILY M~\r\n$ statistical_base_type_code  <chr> \"MAX\", \"AVG\", \"MAX\", \"AVG\", \"MAX~\r\n$ statistical_base_type_desc  <chr> \"Maximum\", \"Average\", \"Maximum\",~\r\n$ limit_value_qualifier_code  <chr> \"\", \"<=\", \"\", \"<=\", \"\", \"<=\", \"<~\r\n$ stay_value_nmbr             <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ dmr_event_id                <chr> \"3600838828\", \"3600838828\", \"360~\r\n$ monitoring_period_end_date  <chr> \"01/31/2017\", \"01/31/2017\", \"02/~\r\n$ dmr_form_value_id           <chr> \"3610877673\", \"3610877668\", \"361~\r\n$ value_type_code             <chr> \"Q2\", \"Q1\", \"Q2\", \"Q1\", \"Q2\", \"Q~\r\n$ value_type_desc             <chr> \"Quantity2\", \"Quantity1\", \"Quant~\r\n$ dmr_value_id                <chr> \"3634771542\", \"3634771541\", \"363~\r\n$ dmr_value_nmbr              <chr> \".0878\", \".0603\", \".0709\", \".053~\r\n$ dmr_unit_code               <chr> \"03\", \"03\", \"03\", \"03\", \"03\", \"0~\r\n$ dmr_unit_desc               <chr> \"MGD\", \"MGD\", \"MGD\", \"MGD\", \"MGD~\r\n$ dmr_value_standard_units    <chr> \".0878\", \".0603\", \".0709\", \".053~\r\n$ dmr_value_qualifier_code    <chr> \"=\", \"=\", \"=\", \"=\", \"=\", \"=\", \"=~\r\n$ value_received_date         <chr> \"02/16/2017\", \"02/16/2017\", \"03/~\r\n$ days_late                   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ nodi_code                   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ nodi_desc                   <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ exceedence_pct              <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ npdes_violation_id          <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ violation_code              <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ violation_desc              <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_detection_code          <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_detection_desc          <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_detection_date          <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_resolution_code         <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_resolution_desc         <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ rnc_resolution_date         <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", ~\r\n$ violation_severity          <chr> \"No Violation Identified\", \"No V~\r\n\r\nThe returned data_frame includes a row for each reporting period from the facility and a row for each statistical type (daily average and daily max in this case). Please note, that if you include the available start_date and end_date arguments in the function, they must be entered as \"mm/dd/yyyy\".\r\nUse dplyr to do some data tidying and ggplot2 to make a quick plot of this data:\r\n\r\n\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\n\r\ndf <- df %>%\r\n  mutate(monitoring_period_end_date = as.Date(monitoring_period_end_date, \"%m/%d/%Y\"),\r\n         dmr_value_nmbr = as.numeric(dmr_value_nmbr))\r\n\r\nggplot(df) +\r\n  geom_line(aes(monitoring_period_end_date, dmr_value_nmbr, color = statistical_base_type_desc)) +\r\n  geom_point(aes(monitoring_period_end_date, dmr_value_nmbr, color = statistical_base_type_desc)) +\r\n  scale_y_log10() +\r\n  labs(x = \"Date\", y = \"Flow (MGD)\", \r\n       title = \"Facility Reported Discharge\", \r\n       subtitle = \"Jan-2017 through Aug-2018\",\r\n       caption = \"source: EPA ECHO\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFind facilities and data\r\nSearching ECHO for for permit holders by location or facility characterisitics is really valuable, albeit potentially verbose. The functions available for facility search have a long list of available search arguments. Here I will demonstrate a search by hydrologic unit code, a useful search area for those in hydrology.\r\n\r\n\r\ndf <- echoWaterGetFacilityInfo(p_huc = \"12100401\")\r\ntibble::glimpse(df)\r\n\r\n\r\nRows: 109\r\nColumns: 26\r\n$ CWPName                  <chr> \"3280 FARM\", \"AES DRILLING FLUIDS E~\r\n$ SourceID                 <chr> \"TXG130056\", \"TXR05CX83\", \"TXR1597D~\r\n$ CWPStreet                <chr> \"1297 FM 3280\", \"27088 US 59 RD\", \"~\r\n$ CWPCity                  <chr> \"PALACIOS\", \"EL CAMPO\", \"EL CAMPO\",~\r\n$ CWPState                 <chr> \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\",~\r\n$ CWPStateDistrict         <chr> \"14\", \"\", \"\", \"12\", \"14\", \"14\", \"12~\r\n$ CWPZip                   <chr> \"774651779\", \"77437-9753\", \"77437\",~\r\n$ MasterExternalPermitNmbr <chr> \"TXG130000\", \"TXR050000\", \"TXR15000~\r\n$ RegistryID               <chr> \"110045502639\", \"110070367509\", \"11~\r\n$ CWPCounty                <chr> \"Jackson\", \"\", \"\", \"Matagorda\", \"Ca~\r\n$ CWPEPARegion             <chr> \"06\", \"06\", \"06\", \"06\", \"06\", \"06\",~\r\n$ FacDerivedHuc            <chr> \"12100401\", \"12100401\", \"12100401\",~\r\n$ FacLat                   <dbl> 28.69808, 29.17455, 29.07067, 28.72~\r\n$ FacLong                  <dbl> -96.32411, -96.29908, -96.27350, -9~\r\n$ CWPTotalDesignFlowNmbr   <dbl> NA, NA, NA, NA, 92.320, NA, NA, NA,~\r\n$ CWPActualAverageFlowNmbr <dbl> NA, NA, NA, NA, 92.320, NA, NA, NA,~\r\n$ ReceivingMs4Name         <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",~\r\n$ AssociatedPollutant      <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",~\r\n$ MsgpPermitType           <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",~\r\n$ CWPPermitStatusDesc      <chr> \"Terminated\", \"Admin Continued\", \"E~\r\n$ CWPPermitTypeDesc        <chr> \"General Permit Covered Facility\", ~\r\n$ CWPIssueDate             <date> 2012-03-13, 2016-10-25, 2020-10-05~\r\n$ CWPEffectiveDate         <date> 2012-04-01, 2016-11-01, 2020-11-01~\r\n$ CWPExpirationDate        <date> 2016-04-17, 2021-08-13, 2023-03-04~\r\n$ CWPSNCStatusDate         <date> 2021-03-31, 2021-03-31, 2021-03-31~\r\n$ LastDMRValueRcvdDate     <date> 2014-12-10, NA, NA, 2021-07-01, 20~\r\n\r\nI found 47 permits, some are terminated some active. By default, the facility search function return a data_frame with variables I deemed useful. However, ECHO provides a a whole host of possible variables (in my state, these are often unfortunately left blank). echoWaterGetParams() will return a dataframe with these variable names, description and ColumnID number. Use the qcolumn argument in the search function to specify what variables you would like returned. According to echoWaterGetParams() there are 299 possible return variables. Certain variables are always returned regardless of the qcolumns argument specified. If I want to find all the POTW (public wastewater treatment plants), I need to specify qcolumn 27, which I cfound using the echoWaterGetMeta() function.\r\n\r\n\r\nmeta <- echoWaterGetMeta()\r\ntibble::glimpse(meta)\r\n\r\n\r\nRows: 252\r\nColumns: 6\r\n$ ColumnName  <chr> \"CWP_NAME\", \"SOURCE_ID\", \"CWP_STREET\", \"CWP_CITY~\r\n$ DataType    <chr> \"VARCHAR2\", \"VARCHAR2\", \"VARCHAR2\", \"VARCHAR2\", ~\r\n$ DataLength  <chr> \"200\", \"30\", \"200\", \"100\", \"2\", \"5\", \"10\", \"9\", ~\r\n$ ColumnID    <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10~\r\n$ ObjectName  <chr> \"CWPName\", \"SourceID\", \"CWPStreet\", \"CWPCity\", \"~\r\n$ Description <chr> \"Facility or permit holder name, as maintained i~\r\n\r\n\r\n\r\ndf <- echoWaterGetFacilityInfo(p_huc = \"12100401\", qcolumns = \"1,2,14,23,24,27\")\r\ndf\r\n\r\n\r\n# A tibble: 109 x 6\r\n   CWPName      SourceID FacDerivedHuc FacLat FacLong CWPFacilityType~\r\n   <chr>        <chr>    <chr>          <dbl>   <dbl> <chr>           \r\n 1 3280 FARM    TXG1300~ 12100401        28.7   -96.3 NON-POTW        \r\n 2 AES DRILLIN~ TXR05CX~ 12100401        29.2   -96.3 NON-POTW        \r\n 3 AKTINA SOLAR TXR1597~ 12100401        29.1   -96.3 NON-POTW        \r\n 4 ALAMO CONCR~ TXG1100~ 12100401        28.7   -96.2 NON-POTW        \r\n 5 ALCOA POINT~ TX00047~ 12100401        28.7   -96.6 NON-POTW        \r\n 6 APEKS AQUAC~ TXG1300~ 12100401        28.7   -96.4 NON-POTW        \r\n 7 BLESSING     TXG1300~ 12100401        28.9   -96.3 NON-POTW        \r\n 8 BLESSING TH~ TXR1529~ 12100401        28.9   -96.2 NON-POTW        \r\n 9 BLESSING TO~ TXR1569~ 12100401        28.7   -96.5 NON-POTW        \r\n10 BOCA CHICA ~ TX00982~ 12100401        28.7   -96.4 NON-POTW        \r\n# ... with 99 more rows\r\n\r\nI can use this data_frame and purrr::pmap to retrieve discharge information. I will look up bacteria concentrations this time.\r\n\r\n\r\ndf <- df %>%\r\n  filter(CWPFacilityTypeIndicator == \"POTW\") %>%\r\n  select(p_id = SourceID)\r\n\r\nreports <- downloadDMRs(df, idColumn = p_id,\r\n                        parameter_code = \"51040\",\r\n                        start_date = \"01/01/2010\",\r\n                        end_date = \"12/30/2017\")\r\nglimpse(reports)                         \r\n\r\n\r\nRows: 8\r\nColumns: 2\r\n$ p_id <chr> \"TX0021474\", \"TX0023051\", \"TX0023167\", \"TX0105104\", \"TX~\r\n$ dmr  <list> [<spec_tbl_df[146 x 61]>], [<spec_tbl_df[0 x 61]>], [<~\r\n\r\nVery quickly, we just found all the POTWs that discharge treated wastewater in the watershed of interest and pulled in their reported discharges. Now we can plot the data and call it a day.\r\n\r\n\r\nreports <- reports %>%\r\n  tidyr::unnest(dmr) %>%\r\n  mutate(monitoring_period_end_date = as.Date(monitoring_period_end_date, \"%m/%d/%Y\"),\r\n         dmr_value_nmbr = as.numeric(dmr_value_nmbr))\r\n\r\nggplot(reports) +\r\n  geom_line(aes(monitoring_period_end_date, dmr_value_nmbr, color = p_id), alpha = 0.5) +\r\n  geom_point(aes(monitoring_period_end_date, dmr_value_nmbr, color = p_id), alpha = 0.5) +\r\n  facet_wrap(~statistical_base_short_desc) +\r\n  scale_y_log10() +\r\n  labs(x = \"Date\", y = \"Bacteria (MPN/100mL)\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\n\r\n\r\nggplot(reports) +\r\n  geom_density(aes(dmr_value_nmbr, fill = p_id, color = p_id), alpha = 0.5, trim = TRUE) +\r\n  facet_wrap(~statistical_base_short_desc) +\r\n  scale_x_log10(labels = scales::comma) +\r\n  labs(x = \"Bacteria (MPN/100mL)\", y = \"Density\") +\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nI was able to quickly generate some time series and density plots without ever touching the ECHO online user interface. There is plenty more documentation at:\r\nhttps://mps9506.github.io/echor/index.html\r\nIf you have suggestions or problems, please report it at:\r\nhttps://github.com/mps9506/echor/issues\r\n\r\n\r\n\r\nGibbs, Carole, and Sally S. Simpson. 2009. “Measuring Corporate Environmental Crime Rates: Progress and Problems.” Crime, Law and Social Change 51 (1): 87–107. https://doi.org/10.1007/s10611-008-9145-1.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/introducing_echor/introducing_echor_files/figure-html5/finalplot1-1.png",
    "last_modified": "2021-08-26T07:18:20-05:00",
    "input_file": "introducing_echor.utf8.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/binomial_water_quality/",
    "title": "Binomial Test for Water Quality Compliance",
    "description": "Use the binomial test to evaluate water quality compliance.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-04-26",
    "categories": [],
    "contents": "\r\nIn the previous post, I used a geometric mean to assess water quality compliance. According to EPA guidance, this is appropriate for assessing bacteria levels in water bodies. For other conventional parameters, we can determine compliance when a standards violation is unlikely to occur more than 10% of the time. Using EPA guidance, we have both a test statistic (proportion of exceedances) and rejection region (>10%). We assume that each water quality sample is a sample from the population that represents the water body with unknown probability (\\(p\\)) of exceeding the criterion. Therefore, the null hypothesis:\r\n\\[H_0 : \\pi \\le p_0\\]\r\nwhere \\(p_0\\) is the acceptable exceedance rate and equals 0.1. By transforming measurements below the criterion to 0 (failure), and measurements above the criterion as 1 (success) we can apply a simple binomial test to samples collected during the assessment period to evaluate current compliance. Smith et. al (Smith et al. 2001) discuss the binomial approach in detail.\r\nIn the example below, I am importing grab dissolved oxygen at two stations on water body. Some events utilize two or more samples at varying depths, those samples are averaged to determine the event dissolved oxygen value.\r\n\r\n\r\nlibrary(readr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nfile <- url(\"https://gist.githubusercontent.com/mps9506/274e0debee7e7f1289dac3371ce05d1e/raw/284ff43565d03bd57c59c3151feaa739abd40f2e/1501_DO.txt\")\r\ndf <- read_delim(file, \"|\")\r\ndf <- df %>%\r\n  #select(`RFA(Sample Set ID)/Tag_id`, Segment, `Parameter Code`, Value, `End Date`, `Monitoring Type`) %>%\r\n  mutate(`End Date` = as.Date(`End Date`, \"%m/%d/%Y\")) %>%\r\n  filter(`Monitoring Type` == \"RT\") %>%\r\n  arrange(`End Date`) %>%\r\n  group_by(`End Date`, `Station ID`) %>%\r\n  summarise(Value = mean(Value))\r\nggplot(df) +\r\n  geom_point(aes(x = `End Date`, y = Value)) +\r\n  geom_hline(aes(yintercept = 4)) +\r\n  xlab(\"Sample Date\") + ylab(\"DO (mg/L)\")\r\n\r\n\r\n\r\n\r\nIn order to determine if the waterbody meets the water quality criterion I select the water quality values during the assessment period and use the binom.test function in R. The arguments for binom.test are\r\nbinom.test(x, n, p = 0.5,\r\n           alternative = c(\"two.sided\", \"less\", \"greater\"),\r\n           conf.level = 0.95)\r\nx    number of successes, or a vector of length 2 giving the numbers of successes and failures, respectively.\r\nn    number of trials; ignored if x has length 2.\r\np    hypothesized probability of success.\r\nalternative    indicates the alternative hypothesis and must be one of \"two.sided\", \"greater\" or \"less\". You can specify just the initial letter.\r\nconf.level    confidence level for the returned confidence interval.\r\nSo we need to first count the total number of “successes,” which in this case means water quality exceedances (DO value less than 4 mg/L). Note, that this is slightly different than worded above since we typically think of exceedance as above a water quality standard. We also need to count the total number of trials. Both of these are accomplished using dplyr and the mutate, case_when, and summarise functions.\r\n\r\n\r\nbinomial_df <- df %>%\r\n  filter(`End Date` > as.Date(\"2005-11-30\") & `End Date` < as.Date(\"2012-12-01\")) %>%\r\n  ungroup() %>%\r\n  mutate(\r\n    success = case_when(\r\n      Value < 4 ~ 1,\r\n      Value >= 4 ~ 0\r\n      ))\r\nbinomial_df <- binomial_df %>%\r\n  summarise(n = n(), x = sum(success))\r\nbinomial_df\r\n\r\n\r\n# A tibble: 1 x 2\r\n      n     x\r\n  <int> <dbl>\r\n1    67    15\r\n\r\nSo, now we have 15 exceedances for 67 trials. This matches the values indicated on the TCEQ waterbody assessment report. (Another pdf warning!)\r\nThe hypothesised rate of success is given to us by the water quality standard, 10%. Therefore the null and alternative hypothesis are:\r\n\\[H_0 : \\pi \\le 0.10\\] \\[H_1 : \\pi \\gt 0.10\\]\r\n\r\n\r\nbinom.test(x = binomial_df$x, n = binomial_df$n, p = 0.1, alternative = \"g\")\r\n\r\n\r\n\r\n    Exact binomial test\r\n\r\ndata:  binomial_df$x and binomial_df$n\r\nnumber of successes = 15, number of trials = 67,\r\np-value = 0.002213\r\nalternative hypothesis: true probability of success is greater than 0.1\r\n95 percent confidence interval:\r\n 0.1433629 1.0000000\r\nsample estimates:\r\nprobability of success \r\n             0.2238806 \r\n\r\nUnder this scenario, we accept reject the null hypothesis. The water body is listed as impaired for depressed dissolved oxygen according to water quality standards.\r\nWe can simplify this test by creating a table or graph depicting the number of exceedances required for a listing based on the number of samples collected. In fact, this table is provided in the TCEQ assessment guide. Using qbinom we can create a table listing the number of exceedances leading to an impairment listing:\r\n\r\nThe maximum Type 1 error rate specified in the State of Texas’s assessment guidance is currently 20% for impairment listings.\r\n\r\n\r\nbinom_chart <- tibble(n = 10:100,\r\n                       exceedances = qbinom(1-0.20, size = 10:100, prob = 0.1) + 1)\r\nbinom_chart\r\n\r\n\r\n# A tibble: 91 x 2\r\n       n exceedances\r\n   <int>       <dbl>\r\n 1    10           3\r\n 2    11           3\r\n 3    12           3\r\n 4    13           3\r\n 5    14           3\r\n 6    15           3\r\n 7    16           4\r\n 8    17           4\r\n 9    18           4\r\n10    19           4\r\n# ... with 81 more rows\r\n\r\n\r\n\r\n\r\nI am also interested in visualizing the trends in exceedance probability. Using the same date-based rolling functions in the previous post, I will apply the binomal test to 7-yrs of the most recent data and plot the probability over sampling date.\r\n\r\n\r\nlibrary(purrr)\r\nlibrary(lubridate) ## import lubridate for as.duration and dyears functions\r\n## This function will be applied to each sample,\r\n## return the estimated probability from the previous 7-yrs of data\r\nmyfunc <- function(dates, values, years, i){\r\n  temp <- values[as.duration(dates[i] - dates)/dyears(1) <= years & as.duration(dates[i] - dates)/dyears(1) >= 0]\r\n  df <- data_frame(temp) %>%\r\n    summarise(n = n(), successes = as.integer(sum(temp)))\r\n  \r\n  results <- binom.test(x = df$successes, n = df$n, p = 0.1, alternative = \"g\")\r\n  \r\n  return(results$estimate)\r\n}\r\ndf2 <- df %>%\r\n  ungroup() %>%\r\n  arrange(`End Date`) %>%\r\n  mutate(Success = as.integer(case_when(\r\n    Value < 4 ~ 1,\r\n    Value >= 4 ~ 0\r\n    )))\r\ndf2 <- df2 %>%\r\n  mutate(ep = map_dbl(seq_along(.$`End Date`),\r\n                  ~myfunc(dates = `End Date`, values = Success, years = 7, i = .x)))\r\ndf2\r\n\r\n\r\n# A tibble: 142 x 5\r\n   `End Date` `Station ID` Value Success    ep\r\n   <date>            <dbl> <dbl>   <int> <dbl>\r\n 1 2000-01-03        12515  6.19       0     0\r\n 2 2000-03-14        12515  6.7        0     0\r\n 3 2000-03-16        12515  6.41       0     0\r\n 4 2000-05-03        12515  4.42       0     0\r\n 5 2000-06-15        12515  4.86       0     0\r\n 6 2000-07-11        12515  4.48       0     0\r\n 7 2000-09-12        12515  5.64       0     0\r\n 8 2000-10-18        12515  8.82       0     0\r\n 9 2000-11-06        12515  5.96       0     0\r\n10 2000-12-20        12515 12.5        0     0\r\n# ... with 132 more rows\r\n\r\nA little ggplot to create the figure:\r\n\r\n\r\nggplot(df2) +\r\n  geom_step(aes(x = `End Date`, y = ep, color = \"7-yr estimated probability of exceedance\")) +\r\n  geom_hline(aes(yintercept = 0.1, color = \"Allowable proportion of exceedances\"), size = .75) +\r\n  theme_ipsum_rc() +\r\n  scale_color_brewer(name = \"\", type = \"qual\", palette = \"Set2\") +\r\n  labs(\r\n    title = \"Estimated probability of exceeding water quality standard\",\r\n    subtitle = \"Based on dissolved oxygen samples in Tres Palacios Tidal\",\r\n    caption = \"Source: TCEQ CRP Data Tool\",\r\n    x = \"Sample date\", y = \"Estimated probability\"\r\n    )\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSmith, Eric P, Keying Ye, Chris Hughes, and Leonard Shabman. 2001. “Statistical Assessment of Violations of Water Quality Standards Under Section 303 (d) of the Clean Water Act.” Environmental Science & Technology 35 (3): 606–12. https://doi.org/10.1021/es001159e.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/binomial_water_quality/binomial_water_quality_files/figure-html5/output-1.png",
    "last_modified": "2021-08-26T07:16:19-05:00",
    "input_file": {},
    "preview_width": 1536,
    "preview_height": 768
  },
  {
    "path": "posts/date_based_rolling/",
    "title": "Date-based rolling functions",
    "description": "Apply rolling statistics to non-routine time series data.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "michaelpaulschramm.com"
      }
    ],
    "date": "2018-04-04",
    "categories": [],
    "contents": "\r\nStates are responsible for water quality assessments that ensure waterbodies comply with designated uses under Section 305(b) of the Clean Water Act. Waterbodies that do not meet applicable standards are listed on the Section 303(d) list, which requires state establish a Total Maximum Daily Load (TMDL) for pollutants responsible for impairment.\r\n\r\nPlenty of reading from EPA regarding establishing and assessing recreational water quality criteria and guidance for assessing water quality data\r\nFor conventional parameters, the current guidance from EPA requires that a waterbody be listed when greater than 10% of samples exceed the numeric criteria. For bacteria parameters, a waterbody is listed when the geometric mean of samples exceeds the criteria. In the state of Texas, the Texas Comission on Environmental Quality (TCEQ) publishes its assessment guidance document (pdf warning) regarding listing and delisting decisions made for waterbodies. TCEQ publishes the results of their assessment every two years in the Texas Integrated Report of Surface Water Quality. Although published every two years, the data typically lags behind an additional two years. For example the 2014 report, which was released in late 2015, includes data collected through 2012. To provide an up to date snapshot, I am interested in providing a visualization of current water quality data and representation of the assessment.\r\nFor bacteria parameters, this seems fairly easy. Plotting a 7-yr rolling geometric mean depicts when assessment exceedances occur. However, most packages in R will calculate rolling averages or functions for regularly spaced data based on the number of observations. Water quality data is collected at random and unequal intervals. For any 7-yr period there might be 20 samples or 100 samples. All valid samples should be included in the function window.\r\n\r\nFor examples using time window rolling functions see zoo, tibbletime, and dplyr::rollsum\r\nRolling geometric mean for bacteria assessment\r\nMy sample dataset is pipe delimited text file obtained from TCEQ’s CRP data tool. It includes Enterococcus bacteria concentrations measured in the Tres Palacios water body. Below is a code chunk I used to download, read, and filter the dataset to something usable.\r\n\r\nSpecifically, I changed the date variable from character to date. I also filter any Monitoring Type values that don’t equal RT, since only RT values are used in assessments (Indicating normal, routine random samples that are not flow or event biased.\r\n\r\n\r\nlibrary(readr)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nfile <- url(\"https://gist.githubusercontent.com/mps9506/004624b5aa9bdf101c36278835cb38df/raw/46267d403bb450da4f7a0c726bd77d4fff1c5be5/1501_Bacteria.txt\")\r\ndf <- read_delim(file, \"|\")\r\ndf <- df %>%\r\n  select(`RFA(Sample Set ID)/Tag_id`, Segment, `Parameter Code`, Value, `End Date`, `Monitoring Type`) %>%\r\n  mutate(`End Date` = as.Date(`End Date`, \"%m/%d/%Y\")) %>%\r\n  filter(`Monitoring Type` == \"RT\")\r\n## Take a quick peek at the data\r\nggplot(df) +\r\n  geom_point(aes(`End Date`, Value)) +\r\n  scale_y_log10() + ylab(\"MPN/100mL\") + xlab(\"Sample Date\")\r\n\r\n\r\nFirst thing that I notice is the number of censored values at 10 MPN/100mL. As far as I am aware, these are left alone for assessemnt purposes. I will revisit this in another post.\r\nIn order to calculate the geometric mean, we need to import a library or define a function since there is no geometric mean function defined in R.\r\n\r\n\r\ngm_mean <- function(x, na.rm=TRUE, zero.propagate = FALSE){\r\n  if(any(x < 0, na.rm = TRUE)){\r\n    return(NaN)\r\n  }\r\n  if(zero.propagate){\r\n    if(any(x == 0, na.rm = TRUE)){\r\n      return(0)\r\n    }\r\n    exp(mean(log(x), na.rm = na.rm))\r\n  } else {\r\n    exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))\r\n  }\r\n}\r\n\r\nI want to utilize purrr::map to apply this function so that it calculates the geometric mean of the last seven years of data from each row. So we need a function that will subtract the dates, identify rows within 7 years of the current row, and return a geometric mean of those rows. We can do this with a loop, but as I am trying to use these map functions provided in the purrr package as they provide a nice functional programming way of addressing this problem.\r\nSo the first step is to create a function that identifies the values within 7 years of the current row and returns the geomean of those values. I also do not need a value calculated for measurements within the first 7 years.\r\n\r\n\r\nlibrary(lubridate) ## import lubridate for as.duration and dyears functions\r\nmyfunc <- function(dates, values, years, i){\r\n  mu <- values[as.duration(dates[i] - dates)/dyears(1) <= years & as.duration(dates[i] - dates)/dyears(1) >= 0]\r\n  if(as.duration(dates[i] - dates[1])/dyears(1) < 7){\r\n    return(NA)\r\n  }\r\n  else(gm_mean(mu)\r\n  )\r\n}\r\n\r\nNow apply this function using map in the dplyr chain:\r\n\r\n\r\nlibrary(purrr)\r\ndf2 <- df %>%\r\n  arrange(`End Date`) %>%\r\n  mutate(Rolled_gm = map_dbl(seq_along(.$`End Date`),\r\n                             ~myfunc(dates = `End Date`, values = `Value`, years = 7, i = .x)))\r\nhead(df2)\r\n\r\n# A tibble: 6 x 7\r\n  `RFA(Sample Set~ Segment `Parameter Code` Value `End Date`\r\n  <chr>              <dbl>            <dbl> <dbl> <date>    \r\n1 R194284             1501            31701    86 2001-03-14\r\n2 R195801             1501            31701    85 2001-06-21\r\n3 L064180             1501            31701   410 2001-09-05\r\n4 R198132             1501            31701   132 2001-09-18\r\n5 L067079             1501            31701   221 2001-11-26\r\n6 R200081             1501            31701   428 2002-01-10\r\n# ... with 2 more variables: `Monitoring Type` <chr>, Rolled_gm <dbl>\r\n\r\nVisualize this with ggplot:\r\n\r\n\r\nlibrary(hrbrthemes)\r\nggplot(df2) +\r\n  geom_point(aes(`End Date`, Value), alpha = 0.25) +\r\n  geom_line(aes(`End Date`, `Rolled_gm`, color = \"7-yr rolling geometric mean\"), size = .75) +\r\n  geom_hline(aes(yintercept = 35, color = \"35 MPN/100mL water quality standard\"), size = .75) +\r\n  scale_y_log10(labels = scales::comma) +\r\n  theme_ipsum_rc() + scale_color_brewer(name = \"\", type = \"qual\", palette = \"Set2\") +\r\n  labs(\r\n    title = \"Enterococcus concentrations\",\r\n    subtitle = \"Tres Palacios Tidal\",\r\n    caption = \"Source: TCEQ CRP Data Tool\",\r\n    x = \"Sample date\", y = \"Concentration (MPN/100mL)\"\r\n  )\r\n\r\n\r\nI think this a relatively easy figure for stakeholders to understand. The red line depicts the rolling geometric mean used to assess compliance with water quality standards at any given sampling point.\r\n\r\n\r\n",
    "preview": "posts/date_based_rolling/date_based_rolling_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/txwater_retweets/",
    "title": "txwater retweets",
    "description": "Let's find out the retweet relationships for txwater twitter users.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2018-02-21",
    "categories": [],
    "contents": "\r\nNote, post updated on July 29, 2020.\r\nLet’s find out the retweet relationships for #txwater twitter users. I’m going to use the R recipes by Bob Rudis in 21 Recipes for Mining Twitter Data with rtweet.\r\nExtract the origin\r\n\r\n\r\nlibrary(rtweet)\r\nlibrary(tidyverse)\r\ntoken <- readRDS(Sys.getenv(\"TWITTER_PAT\")) ## see https://docs.ropensci.org/rtweet/articles/auth.html\r\ntxwater <- search_tweets(\"#txwater\", \r\n                         n = 1000, \r\n                         token = token)\r\noutput <- filter(txwater, \r\n                 retweet_count > 0) %>% \r\n  select(text, \r\n         mentions_screen_name, \r\n         retweet_count) %>% \r\n  mutate(text = substr(text, 1, 30)) %>% \r\n  unnest()\r\nas_tibble(output)\r\n\r\n# A tibble: 378 x 3\r\n   text                     mentions_screen_n~ retweet_count\r\n   <chr>                    <chr>                      <int>\r\n 1 \"We appreciate the oppo~ TexasPlusWater                 2\r\n 2 \"We appreciate the oppo~ txawwa                         2\r\n 3 \"We appreciate the oppo~ TexasWCA                       2\r\n 4 \"Houston Officials Are ~ <NA>                           2\r\n 5 \"This issue of @MuniWat~ TexasWCA                       5\r\n 6 \"This issue of @MuniWat~ MuniWaterLeader                5\r\n 7 \"This issue of @MuniWat~ NTMWD                          5\r\n 8 \"This issue of @MuniWat~ MySAWS                         5\r\n 9 \"The Texas Parks and Wi~ <NA>                           2\r\n10 \"Frankly the change in ~ PLFTX                          1\r\n# ... with 368 more rows\r\n\r\nPlot txwater retweet degree distribution\r\n\r\n\r\nlibrary(igraph)\r\nlibrary(hrbrthemes)\r\nrt_g <- filter(txwater, retweet_count > 0) %>% \r\n  select(screen_name, mentions_screen_name) %>%\r\n  unnest(mentions_screen_name) %>% \r\n  filter(!is.na(mentions_screen_name)) %>% \r\n  graph_from_data_frame()\r\nggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) +\r\n  geom_segment(aes(x, y, xend=x, yend=0), color=\"slateblue\") +\r\n  scale_y_continuous(expand=c(0,0), trans=\"sqrt\") +\r\n  labs(x=\"Degree\", y=\"Density (sqrt scale)\", title=\"#txwater Retweet Degree Distribution\") +\r\n  theme_ipsum_rc(grid=\"Y\", axis=\"x\")\r\n\r\n\r\nPlot retweet relationships\r\n\r\n\r\nlibrary(ggraph)\r\n# Label nodes\r\nV(rt_g)$node_label <- unname(names(V(rt_g)))\r\n# Size of node\r\nV(rt_g)$node_size <- unname(ifelse(degree(rt_g)[V(rt_g)] > 1, degree(rt_g), 1)) \r\n# Adjust angle of label based on position\r\nnIds <- length(V(rt_g))\r\nV(rt_g)$Id <- seq(1:nIds)\r\nV(rt_g)$label_angle <- 90 - 360 *  V(rt_g)$Id / nIds\r\nV(rt_g)$hjust <- ifelse(V(rt_g)$label_angle < -90, 1, 0)\r\n# Flip text depending on what side of the plot it is on\r\nV(rt_g)$angle <- ifelse(V(rt_g)$label_angle < -90, V(rt_g)$label_angle+180, V(rt_g)$label_angle)\r\np <- ggraph(rt_g, layout = 'linear', circular = TRUE) + \r\n  geom_edge_arc(aes(alpha=..index..)) +\r\n  geom_node_point(aes(x = x*1.07, y=y*1.07, size=node_size,  alpha=0.2)) +\r\n  geom_node_text(aes(x=x*1.15, y=y*1.15,label=node_label, angle=angle, hjust=hjust),\r\n                  color=\"dodgerblue\", size=2.7, family=font_rc) +\r\n  coord_fixed() +\r\n  labs(title=\"#txwater Relationships\", subtitle=\"Darkers edges == more retweets. Node size == larger degree\") +\r\n  theme_graph(base_family=font_rc) +\r\n  theme(legend.position=\"none\") +\r\n  expand_limits(x = c(-1.3, 1.3), y = c(-1.3, 1.3))\r\np\r\n\r\n\r\nThe rtweet search_tweets() function returns approximately 8 days worth of retweets. Depending on when this script was run, the relationships might change. I was suprised to see only a handful of state agencies are well engaged in this hashtag. The next step is to take look at relationships between users and text sentiment analysis. But that is it for this post.\r\n\r\n\r\n",
    "preview": "posts/txwater_retweets/txwater_retweets_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {},
    "preview_width": 3200,
    "preview_height": 3200
  },
  {
    "path": "posts/time-series-python/",
    "title": "Time-series decomposition and trend analysis in Python",
    "description": "Decompose time series in Python and a function for the Mann-Kendall test for trend.",
    "author": [
      {
        "name": "Michael Schramm",
        "url": "https://michaelpaulschramm.com"
      }
    ],
    "date": "2015-08-01",
    "categories": [],
    "contents": "\r\nThere are a number of methods to accomplish time-series decompositions in R, including the decompose and STL commands.\r\nI haven’t come across a seasonal decomposition method in Python comparable to R’s STL. However, statsmodels 0.6 added a naive seasonal decomposition method similar to R’s decompose that is not as powerful as the LOESS method used in STL. Let’s run through an example:\r\n\r\nimport urllib2  \r\nimport datetime as datetime  \r\nimport pandas as pd  \r\nimport statsmodels.api as sm  \r\nimport seaborn as sns  \r\nimport matplotlib.pyplot as plt\r\n\r\n# Import the sample streamflow dataset\r\ndata = urllib2.urlopen('https://raw.github.com/mps9506/Sample-Datasets/master/Streamflow/USGS-Monthly_Streamflow_Bend_OR.tsv')  \r\ndf = pd.read_csv(data, sep='\\t')\r\n\r\n# The yyyy,mm, and dd are in seperate columns, we need to make this a single column\r\ndf['dti'] = df[['year_nu','month_nu','dd_nu']].apply(lambda x: datetime.datetime(*x),axis=1)\r\n\r\n# Let use this as our index since we are using pandas\r\ndf.index = pd.DatetimeIndex(df['dti'])  \r\n# Clean the dataframe a bit\r\ndf = df.drop(['dd_nu','year_nu','month_nu','dti'],axis=1)  \r\ndf = df.resample('M',how='mean')  \r\nprint df.head()  \r\nfig,ax = plt.subplots(1,1, figsize=(6,4))  \r\nflow = df['mean_va']  \r\nflow = flow['1949-01':]\r\n\r\nres = sm.tsa.seasonal_decompose(flow)  \r\nfig = res.plot()  \r\nfig.show()  \r\ndecomposeEach component can then be accessed with:\r\n\r\nresidual = res.residual\r\nseasonal = res.seasonal\r\ntrend = res.trend\r\nprint trend['1950':'1951']\r\n\r\n1950-01-31    1441.591667  \r\n1950-02-28    1468.133333  \r\n1950-03-31    1499.883333  \r\n1950-04-30    1521.466667  \r\n1950-05-31    1540.633333  \r\n1950-06-30    1572.079167  \r\n1950-07-31    1611.412500  \r\n1950-08-31    1666.541667  \r\n1950-09-30    1720.658333  \r\n1950-10-31    1759.700000  \r\n1950-11-30    1780.408333  \r\n1950-12-31    1789.491667  \r\n1951-01-31    1800.950000  \r\n1951-02-28    1810.950000  \r\n1951-03-31    1819.616667  \r\n1951-04-30    1848.866667  \r\n1951-05-31    1889.850000  \r\n1951-06-30    1895.979167  \r\n1951-07-31    1878.858333  \r\n1951-08-31    1841.137500  \r\n1951-09-30    1806.308333  \r\n1951-10-31    1807.850000  \r\n1951-11-30    1826.516667  \r\n1951-12-31    1856.683333  \r\nFreq: M, Name: mean_va, dtype: float64  \r\nIf we want to determine if there is a simple monotonic trend in this data we can utilize the Mann-Kendall test for trend. This doesn’t appear to be available in scipy.stats or statsmodels yet. I came across a function written by Sat Kumar Tomer, the homepage with the software package seems to be gone, so I verified the output to implementations in R and uploaded to GitHub so it won’t disappear.\r\n\r\nimport numpy as np  \r\nfrom scipy.stats import norm, mstats\r\n\r\n\r\ndef mk_test(x, alpha = 0.05):  \r\n    \"\"\"   \r\n    Input:\r\n        x:   a vector of data\r\n        alpha: significance level (0.05 default)\r\n\r\n    Output:\r\n        trend: tells the trend (increasing, decreasing or no trend)\r\n        h: True (if trend is present) or False (if trend is absence)\r\n        p: p value of the significance test\r\n        z: normalized test statistics \r\n\r\n    Examples\r\n    --------\r\n      >>> x = np.random.rand(100)\r\n      >>> trend,h,p,z = mk_test(x,0.05) \r\n    \"\"\"\r\n    n = len(x)\r\n\r\n    # calculate S \r\n    s = 0\r\n    for k in range(n-1):\r\n        for j in range(k+1,n):\r\n            s += np.sign(x[j] - x[k])\r\n\r\n    # calculate the unique data\r\n    unique_x = np.unique(x)\r\n    g = len(unique_x)\r\n\r\n    # calculate the var(s)\r\n    if n == g: # there is no tie\r\n        var_s = (n*(n-1)*(2*n+5))/18\r\n    else: # there are some ties in data\r\n        tp = np.zeros(unique_x.shape)\r\n        for i in range(len(unique_x)):\r\n            tp[i] = sum(unique_x[i] == x)\r\n        var_s = (n*(n-1)*(2*n+5) + np.sum(tp*(tp-1)*(2*tp+5)))/18\r\n\r\n    if s>0:\r\n        z = (s - 1)/np.sqrt(var_s)\r\n    elif s == 0:\r\n            z = 0\r\n    elif s<0:\r\n        z = (s + 1)/np.sqrt(var_s)\r\n\r\n    # calculate the p_value\r\n    p = 2*(1-norm.cdf(abs(z))) # two tail test\r\n    h = abs(z) > norm.ppf(1-alpha/2) \r\n\r\n    if (z<0) and h:\r\n        trend = 'decreasing'\r\n    elif (z>0) and h:\r\n        trend = 'increasing'\r\n    else:\r\n        trend = 'no trend'\r\n\r\n    return trend, h, p, z\r\nLet’s see if there is a trend direction in the first decade of data 1950-1960:\r\n\r\ntrend = res.trend['1950':'1960']  \r\ntest_trend,h,p,z = mk_test(trend,alpha=0.05)  \r\nprint test_trend, h  \r\nprint z, p  \r\n\r\ndecreasing True  \r\n-4.05429896945 5.02848722452e-05\r\nThe test indicates a monotonic decreasing trend over the time period, with a Mann-Kendall Z stat = -4.05 and p<0.05.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-08-07T22:33:14-05:00",
    "input_file": {}
  }
]
